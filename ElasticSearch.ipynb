{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "\n",
    "class MySearch:\n",
    "    max_id = 0\n",
    "    \n",
    "    def __init__(self, index='paper_index', addr='http://localhost:9200'):\n",
    "        self.es = Elasticsearch(addr)\n",
    "        self.index = index\n",
    "        \n",
    "    def index_json(self, path):\n",
    "        with open(path, 'r') as my_file:\n",
    "            data = json.load(my_file)\n",
    "            for i, paper in enumerate(data):\n",
    "                if paper['type'] == 'paper':\n",
    "                    paper.pop('type')\n",
    "                    self.es.index(index=self.index, doc_type=self.index, id=MySearch.max_id+1, body={'paper': paper})\n",
    "                    MySearch.max_id +=1\n",
    "        return True\n",
    "    \n",
    "    def delete_index(self):\n",
    "        return self.es.indices.delete(self.index)\n",
    "        \n",
    "    def bare_search(self, query, size=10):\n",
    "        return self.es.search(index=self.index, doc_type=self.index, body=query, size=size)\n",
    "    \n",
    "    def update(self, id, new_dict):\n",
    "        return self.es.update(index=self.index,doc_type=self.index,id=id, body={\"doc\": new_dict})\n",
    "    \n",
    "    def search(self, title_query, abstract_query, year, w_title=1.0,\n",
    "               w_abstract=1.0, w_date=1.0, use_page_rank=False, size = 10):\n",
    "        my_query = {\"query\": { \n",
    "                \"bool\": {\n",
    "                    \"should\": [\n",
    "                        {\"match\" : {\n",
    "                          \"paper.title\":{\n",
    "                              \"query\":title_query,\n",
    "                              \"boost\": w_title\n",
    "                        }}},\n",
    "                        {\"match\" : {\n",
    "                          \"paper.abstract\":{\n",
    "                              \"query\":abstract_query,\n",
    "                              \"boost\": w_abstract\n",
    "                        }}},\n",
    "                        { \"range\": { \"paper.date\": { \"gte\": str(year) , \"boost\" : w_date}}}\n",
    "                    ]\n",
    "                }\n",
    "            }}\n",
    "        if not use_page_rank:\n",
    "            return self.bare_search(my_query, size)\n",
    "        else:\n",
    "            new_query = {\n",
    "                        \"query\": {\n",
    "                            \"function_score\": {\n",
    "                                \"query\": my_query[\"query\"],\n",
    "                                \"script_score\" : {\n",
    "                                    \"script\" : {\n",
    "                                      \"source\": \"_score + doc['paper.page_rank'].value\"\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        }}\n",
    "            return self.bare_search(new_query, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = MySearch()\n",
    "# ms.index_json('semanticCrawler/data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ms.delete_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_shards': {'failed': 0, 'skipped': 0, 'successful': 5, 'total': 5},\n",
       " 'hits': {'hits': [{'_id': '663',\n",
       "    '_index': 'paper_index',\n",
       "    '_score': 1.9333732,\n",
       "    '_source': {'paper': {'abstract': 'Semantic Scholar extracted view of \"Coherence and Coreference\" by Jerry R. Hobbs',\n",
       "      'authors': ['Jerry R. Hobbs'],\n",
       "      'date': '1979',\n",
       "      'id': 'Coherence-and-Coreference-Hobbs/e564391324ede7c9771e78b6d8c23bee5afff559',\n",
       "      'page_rank': 0.00020358794857695006,\n",
       "      'references': [],\n",
       "      'title': 'Coherence and Coreference'}},\n",
       "    '_type': 'paper_index'}],\n",
       "  'max_score': 1.9333732,\n",
       "  'total': 566},\n",
       " 'timed_out': False,\n",
       " 'took': 5}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms.bare_search({'query':{'match':{'paper.title':'and'}}}, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "def pageRank(G, s = .85, maxerr = .0001):\n",
    "    n = G.shape[0]\n",
    "    A = csc_matrix(G,dtype=np.float)\n",
    "    rsums = np.array(A.sum(1))[:,0]\n",
    "    ri, ci = A.nonzero()\n",
    "    A.data /= rsums[ri]\n",
    "\n",
    "    ro, r = np.zeros(n), np.ones(n)\n",
    "    while np.sum(np.abs(r-ro)) > maxerr:\n",
    "        print('ERR:', np.sum(np.abs(r-ro)))\n",
    "        ro = r.copy()\n",
    "        for i in range(0,n):\n",
    "            Ai = np.array(A[:,i].todense())[:,0]\n",
    "            Ei = np.ones(n) / float(n)\n",
    "            r[i] = ro.dot( Ai*s + Ei*(1-s))\n",
    "            \n",
    "    return r/float(sum(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(path):\n",
    "    ids = []\n",
    "    graph = []\n",
    "    authors = set()\n",
    "    authors_graph = []\n",
    "    with open(path, 'r') as my_file:\n",
    "        data = json.load(my_file)\n",
    "        new_data = []\n",
    "        for paper in data:\n",
    "            if paper['type'] == 'paper':\n",
    "                new_data.append(paper)\n",
    "                ids.append(paper['id'])\n",
    "                for author in paper['authors']:\n",
    "                    authors.add(author)\n",
    "\n",
    "        data = new_data\n",
    "        authors = list(authors)\n",
    "        authors_index ={author:i for i, author in enumerate(authors)}\n",
    "        for i in range(len(authors)):\n",
    "            authors_graph.append([])\n",
    "            for j in range(len(authors)):\n",
    "                authors_graph[i].append(0)\n",
    "            \n",
    "        for ind1, paper1 in enumerate(data):\n",
    "            graph.append([])\n",
    "            for ind2, paper2 in enumerate(data):\n",
    "                if paper2['id'] in paper1['references']:\n",
    "                    graph[ind1].append(1)\n",
    "                    \n",
    "                    for author1 in paper1['authors']:\n",
    "                        for author2 in paper2['authors']:\n",
    "                            authors_graph[authors_index[author1]][authors_index[author2]] = 1\n",
    "                else:\n",
    "                    graph[ind1].append(0)\n",
    "    \n",
    "    return graph, ids, authors_graph, authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph, ids , authors_graph, authors= create_graph('semanticCrawler/data.json')\n",
    "graph = np.array(graph, dtype=np.float)\n",
    "authors_graph = np.array(authors_graph, dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERR: 2000.0\n",
      "ERR: 1156.8520238095234\n",
      "ERR: 476.46353660863895\n",
      "ERR: 229.65400258650843\n",
      "ERR: 112.62301365575024\n",
      "ERR: 56.98554497768786\n",
      "ERR: 29.032707209362307\n",
      "ERR: 14.750073802979609\n",
      "ERR: 7.461954984207983\n",
      "ERR: 3.8024358769042843\n",
      "ERR: 1.935596239438159\n",
      "ERR: 0.9959575075906749\n",
      "ERR: 0.5112930186440392\n",
      "ERR: 0.2636425033725379\n",
      "ERR: 0.1359713471133596\n",
      "ERR: 0.07002016997793546\n",
      "ERR: 0.0360416882318603\n",
      "ERR: 0.018526308021111104\n",
      "ERR: 0.009517107780170078\n",
      "ERR: 0.0048848875597231715\n",
      "ERR: 0.0025067513316527662\n",
      "ERR: 0.001285984903271673\n",
      "ERR: 0.0006597065558292028\n",
      "ERR: 0.00033840719178822553\n",
      "ERR: 0.00017359993900468228\n"
     ]
    }
   ],
   "source": [
    "pr = pageRank(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, p in enumerate(pr):\n",
    "    ms.update(i+1, {'paper': {'page_rank':p}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_shards': {'failed': 0, 'skipped': 0, 'successful': 5, 'total': 5},\n",
       " 'hits': {'hits': [{'_id': '5',\n",
       "    '_index': 'paper_index',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'paper': {'abstract': 'Big-data server applications frequently encounter data misses, and hence, lose significant performance potential. One way to reduce the number of data misses or their effect is data prefetching. As data accesses have high temporal correlations, temporal prefetching techniques are promising for them. While state-of-the-art temporal prefetching techniques are effective at reducing the number of data misses, we observe that there is a significant gap between what they offer and the opportunity. This work aims to improve the effectiveness of temporal prefetching techniques. We identify the lookup mechanism of existing temporal prefetchers responsible for the large gap between what they offer and the opportunity. Existing lookup mechanisms either not choose the right stream in the history, or unnecessarily delay the stream selection, and hence, miss the opportunity at the beginning of every stream. In this work, we introduce Domino prefetching to address the limitations of existing temporal prefetchers. Domino prefetcher is a temporal data prefetching technique that logically looks up the history with both one and two last miss addresses to find a match for prefetching. We propose a practical design for Domino prefetcher that employs an Enhanced Index Table that is indexed by just a single miss address. We show that Domino prefetcher captures more than 90% of the temporal opportunity. Through detailed evaluation targeting a quad-core processor and a set of server workloads, we show that Domino prefetcher improves system performance by 16% over the baseline with no data prefetcher and 6% over the state-of- the-art temporal data prefetcher.',\n",
       "      'authors': ['Mohammad Bakhshalipour',\n",
       "       'Pejman Lotfi-Kamran',\n",
       "       'Hamid Sarbazi-Azad'],\n",
       "      'date': '2018',\n",
       "      'id': 'Domino-Temporal-Data-Prefetcher-Bakhshalipour-Lotfi-Kamran/665c0dde22c2f8598869d690d59c9b6d84b07c01',\n",
       "      'page_rank': 0.00014618441055616897,\n",
       "      'references': ['Identifying-Hierarchical-Structure-in-Sequences%3A-A-Nevill-Manning-Witten/02aca8223525caa99efc4b0e2810e450ee6776ba',\n",
       "       'Clearing-the-clouds%3A-a-study-of-emerging-scale-out-Ferdman-Adileh/8b10b13fb495101d1e4eb768907cff05e3bd9315',\n",
       "       'Practical-off-chip-meta-data-for-temporal-memory-Wenisch-Ferdman/2b51020e54a6361316cc477c39a2759902022580',\n",
       "       'Temporal-memory-streaming-Wenisch/adc43c28e86d5f577e3b4b2ff120162a7225b1f0',\n",
       "       'Data-cache-prefetching-using-a-global-history-Nesbit-Smith/3933d3380710680865441c87a22d691ba751a2ef',\n",
       "       'Performance-of-Database-Workloads-on-Shared-Memory-Ranganathan-Gharachorloo/04dba1bd51e8f3348c57cb7b5148abd9f4b5aa21',\n",
       "       'Linearizing-irregular-memory-accesses-for-improved-Jain-Lin/2aed1e5dab68713bf8ed0fb4b35a0ec67b087c30',\n",
       "       'Spatio-temporal-memory-streaming-Somogyi-Wenisch/048336a0dc1029416ce47c78b9a5cba8422e6efd',\n",
       "       'Temporal-streams-in-commercial-server-applications-Wenisch-Ferdman/293e7c79a37798032b3047324d101bc49d9a37a2',\n",
       "       'Dynamic-Hot-Data-Stream-Prefetching-for-Programs-Chilimbi-Hirzel/60b85b7ee655397a4d2202f9cdf6dd5e3f04f6fd'],\n",
       "      'title': 'Domino Temporal Data Prefetcher'}},\n",
       "    '_type': 'paper_index'}],\n",
       "  'max_score': 1.0,\n",
       "  'total': 19},\n",
       " 'timed_out': False,\n",
       " 'took': 9}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms.search('coordinate', 'coordinate', 2018, 1.0, 0.0, 1.0, False, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(v):\n",
    "    norm = sum(v)\n",
    "    if norm == 0: \n",
    "        return v\n",
    "    return v / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HITS(graph, epochs=5):\n",
    "    a = np.ones(len(graph))\n",
    "    h = np.ones(len(graph))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        h = graph.dot(a)\n",
    "        a = h.dot(graph)\n",
    "        a = normalize(a)\n",
    "        h = normalize(h)\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_k(authors_graph, authors, k):\n",
    "    a = HITS(authors_graph)\n",
    "    authorities = [x for x in zip(a, authors)]\n",
    "    authorities.sort(reverse=True)\n",
    "    return authorities[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.012438237419841238, 'Torsten N. Wiesel'),\n",
       " (0.011940380560591296, 'David H. Hubel'),\n",
       " (0.009335599102681308, 'Peter H. Schiller'),\n",
       " (0.008338461129592932, 'Max S. Cynader'),\n",
       " (0.008239153214949906, 'Nancy Berman'),\n",
       " (0.00785247232450169, 'Charles D. Gilbert'),\n",
       " (0.007708886458653803, 'Michael P. Stryker'),\n",
       " (0.007611779177375818, 'F. W. Campbell'),\n",
       " (0.00751187564514074, 'Raymond D. Lund'),\n",
       " (0.007457760850912143, 'Simon Levay')]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_best_k(authors_graph, authors, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46e4aa9e1134afd9bfbc0a82e11e629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>Text</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "Text(value='', description='File path')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31a2e9aa425440ab33b62299a9af03c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>Button</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "Button(description='Index', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaee37372efa42948b6a34c5b415bbfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>Button</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "Button(description='Delete index', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47cc0ac4e1254bc6bfec3dd95fab0642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>Output</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display,clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "path_txt = widgets.Text(description=\"File path\", width=200)\n",
    "index_btn = widgets.Button(description='Index')\n",
    "delete_btn = widgets.Button(description='Delete index')\n",
    "out = widgets.Output()\n",
    "\n",
    "def index_btn_click(event):\n",
    "    with out:\n",
    "        clear_output(wait=False)\n",
    "        if not path_txt.value:\n",
    "            print('Error: Empty path!')\n",
    "        else:\n",
    "            print('Indexing...')\n",
    "            if ms.index_json(path_txt.value):\n",
    "                print('Done')\n",
    "            else:\n",
    "                print('Error!')\n",
    "    \n",
    "def delete_btn_click(event):\n",
    "    with out:\n",
    "        clear_output(wait=False)\n",
    "        print(ms.delete_index())\n",
    "\n",
    "index_btn.on_click(index_btn_click)\n",
    "delete_btn.on_click(delete_btn_click)\n",
    "display(path_txt)\n",
    "display(index_btn)\n",
    "display(delete_btn)\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
